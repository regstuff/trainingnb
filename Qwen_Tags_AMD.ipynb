{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4225ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a670f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "max_seq_length = 2048\n",
    "model_name = \"unsloth/Qwen3-1.7B\"\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length, # Choose any for long context!\n",
    "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")\n",
    "\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0.05, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 42,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b9a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "url = \"https://huggingface.co/Reggie/test/resolve/main/tags_finetuning_sample.zip?download=true\"\n",
    "password = b\"unsloth\" # Password in bytes\n",
    "\n",
    "print(\"Downloading file to RAM...\")\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # 1. Wrap the downloaded bytes in BytesIO so ZipFile treats it like a file\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "        \n",
    "        # Find the jsonl file\n",
    "        jsonl_filename = [name for name in z.namelist() if name.endswith('.jsonl')][0]\n",
    "        print(f\"Found {jsonl_filename}, extracting to DataFrame...\")\n",
    "        \n",
    "        # 2. Open the specific file in the zip stream\n",
    "        with z.open(jsonl_filename, pwd=password) as f:\n",
    "            # Read directly to Pandas\n",
    "            df = pd.read_json(f, lines=True)\n",
    "\n",
    "    # 3. Convert to Hugging Face Dataset\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    # Cleanup memory\n",
    "    del df\n",
    "    \n",
    "    print(\"Success! Dataset loaded.\")\n",
    "    print(dataset)\n",
    "else:\n",
    "    print(f\"Download failed. Status code: {response.status_code}\")\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.02, seed=42)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "\n",
    "print(f\"‚úÖ Dataset split:\")\n",
    "print(f\"   Training examples: {len(train_dataset)}\")\n",
    "print(f\"   Validation examples: {len(eval_dataset)}\")\n",
    "\n",
    "\n",
    "alpaca_prompt = \"\"\"<|im_start|>user\n",
    "{}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "<think>\n",
    "\n",
    "</think>\n",
    "\n",
    "{}<|im_end|>\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # End-of-sequence token\n",
    "print('EOS TOKEN:', EOS_TOKEN)\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"\n",
    "    Convert Alpaca format to Gemma 3 chat format.\n",
    "\n",
    "    For each example:\n",
    "    1. Combine instruction + input (input is empty for us)\n",
    "    2. Format as Gemma chat turn\n",
    "    3. Add EOS token for proper training\n",
    "    \"\"\"\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        # Combine instruction and input (input is empty for our dataset)\n",
    "        full_instruction = instruction + (\"\\n\" + input_text if input_text else \"\")\n",
    "\n",
    "        # Format as chat turns\n",
    "        text = alpaca_prompt.format(full_instruction, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting to both train and validation sets\n",
    "train_dataset = train_dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Dataset formatted for Qwen 3 chat!\")\n",
    "print(\"\\nüìù Formatted Example:\")\n",
    "print(\"-\" * 80)\n",
    "print(train_dataset[0]['text'])\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    # Output and logging\n",
    "    output_dir=\"./outputs\",              # Where to save model checkpoints\n",
    "    logging_dir=\"./logs\",                # Where to save logs\n",
    "    logging_steps=50,                    # Log every 10 steps\n",
    "\n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,                  # Train for 3 epochs\n",
    "    per_device_train_batch_size=64,       # 4 examples per GPU\n",
    "    gradient_accumulation_steps=1,       # Accumulate 4 batches (effective batch=16)\n",
    "    learning_rate=1e-4,                  # Standard LoRA learning rate\n",
    "    weight_decay=0.01,                   # L2 regularization\n",
    "\n",
    "    # Learning rate schedule\n",
    "    lr_scheduler_type=\"cosine\",          # Cosine decay schedule\n",
    "    warmup_steps=100,                     # Warmup for first 50 steps\n",
    "\n",
    "    # Optimization\n",
    "    optim=\"adamw_torch\",                  # 8-bit AdamW (saves memory)\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    bf16 = is_bfloat16_supported(),      # Mixed precision training (2x faster)\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",               # Evaluate during training\n",
    "    eval_steps=250,                       # Evaluate every 50 steps\n",
    "    per_device_eval_batch_size=64,        # Batch size for evaluation\n",
    "\n",
    "    # Checkpointing\n",
    "    save_strategy=\"steps\",               # Save checkpoints\n",
    "    save_steps=100,                      # Save every 100 steps\n",
    "    save_total_limit=5,                  # Keep only 3 best checkpoints\n",
    "    load_best_model_at_end=True,         # Load best checkpoint at end\n",
    "    metric_for_best_model=\"eval_loss\",   # Use validation loss to pick best\n",
    "\n",
    "    # Memory optimizations\n",
    "    gradient_checkpointing=True,         # Save memory (slight speed cost)\n",
    "    max_grad_norm=1.0,                   # Gradient clipping (stability)\n",
    "\n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    "\n",
    "    # Disable unnecessary features\n",
    "    report_to=\"none\",                    # Don't report to wandb/tensorboard\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration set!\")\n",
    "print(\"\\nüìä Training Summary:\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Warmup steps: {training_args.warmup_steps}\")\n",
    "print(f\"   FP16 enabled: {training_args.fp16}\")\n",
    "\n",
    "# Calculate approximate training time\n",
    "total_steps = (len(train_dataset) * training_args.num_train_epochs) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\n",
    "print(f\"   Total steps: ~{total_steps}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create trainer with SFTTrainer (Supervised Fine-Tuning)\n",
    "from transformers import TrainerCallback, EarlyStoppingCallback\n",
    "import random\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "class MemoryCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Monitors VRAM usage and shows a random evaluation sample\n",
    "    after each evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, eval_dataset, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eval_dataset = eval_dataset\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        self._show_random_sample()\n",
    "\n",
    "    def _show_random_sample(self):\n",
    "        \"\"\"Selects, generates, and prints a random evaluation sample.\"\"\"\n",
    "        try:\n",
    "            # 1. Get random sample\n",
    "            idx = random.randint(0, len(self.eval_dataset) - 1)\n",
    "            sample = self.eval_dataset[idx]\n",
    "            full_text = sample['text'] # Assumes 'text' field\n",
    "            \n",
    "            # 2. Parse prompt and label from your specific format\n",
    "            parts = full_text.split(\"<|im_start|>assistant\\n<think>\\n\\n</think>\\n\")\n",
    "            if len(parts) != 2:\n",
    "                print(f\"   (Skipping sample: unexpected format '{full_text[:50]}...')\")\n",
    "                return\n",
    "                \n",
    "            # Re-create the prompt model expects: \"...<end_of_turn><start_of_turn>model\\n\"\n",
    "            input_prompt = parts[0] + \"<|im_start|>assistant\\n<think>\\n\\n</think>\\n\"\n",
    "            ground_truth = parts[1].split(\"<|im_end|>\")[0].strip()\n",
    "            \n",
    "            # 3. Tokenize and move to device\n",
    "            inputs = self.tokenizer(input_prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "            \n",
    "            # 4. Generate prediction\n",
    "            # Use torch.no_grad() for efficiency\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs, \n",
    "                    max_new_tokens=50, # Or your desired keyword limit\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # 5. Decode *only* the new tokens\n",
    "            # We slice the output tensor to remove the input prompt tokens\n",
    "            generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "            generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "            # 6. Print formatted comparison\n",
    "            print(\"\\n\" + \"=\"*30 + f\" üé≤ Random Eval Sample (Index: {idx}) \" + \"=\"*30)\n",
    "            # Show only the 'user' part of the prompt for clarity\n",
    "            user_prompt = parts[0].replace('<|im_start|>user', '').strip()\n",
    "            print(f\"PROMPT (User Turn):\\n{user_prompt}\")\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"MODEL (Generated):\\n{generated_text}\")\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"TRUTH (Label):\\n{ground_truth}\")\n",
    "            print(\"=\" * 80)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   (Error during random sample generation: {e})\")\n",
    "\n",
    "\n",
    "memory_callback = MemoryCallback(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",  # Which field contains the formatted text\n",
    "    max_seq_length=max_seq_length,\n",
    "    args=training_args,\n",
    "    packing=False,  # Don't pack multiple examples (our examples are short)\n",
    "    callbacks=[early_stopping, memory_callback],\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2fd7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ Training complete!\")\n",
    "print(\"\\nüìä Final Statistics:\")\n",
    "print(f\"   Train runtime: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"   Train samples/second: {trainer_stats.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"   Final train loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
    "\n",
    "# Get validation metrics\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"\\nüìà Validation Results:\")\n",
    "print(f\"   Validation loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(f\"   Validation perplexity: {eval_results.get('eval_perplexity', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69f07a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b976c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inference mode (faster, less memory)\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def test_command_translation(nl_query):\n",
    "    instruction = f\"List the content type of the passage and all relevant tags:\\nPassage: {nl_query}\"\n",
    "\n",
    "    # Format as Qwen3 chat turn\n",
    "\n",
    "    prompt = f\"\"\"<|im_start|>user\n",
    "    {instruction}<|im_end|>\n",
    "  <|im_start|>assistant\n",
    "  <think>\n",
    "\n",
    "  </think>\"\"\"\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.1,  # Low temperature for deterministic output\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract model response (after \"<start_of_turn>model\")\n",
    "    if \"<start_of_turn>model\" in response:\n",
    "        response = response.split(\"<start_of_turn>model\")[-1].strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "print(\"‚úÖ Inference mode enabled!\")\n",
    "\n",
    "\n",
    "# Simple non-format test\n",
    "prompt = \"\"\"<|im_start|>user\n",
    "Hello!<|im_end|>\n",
    "<|im_start|>assistant\n",
    "<think>\n",
    "\n",
    "</think>\"\"\"\n",
    "\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "print(\"# Simple non-format test, full output:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"Without special tokens:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "print('Proper test with right formatting')\n",
    "\n",
    "for text in [\"\"\"\"I must tell you my experience. There was a time I crisscrossed India on my motorcycle. Once again, I'm getting back on my motorcycle these days (Laughs). So I'm just riding somewhere between Madhya Pradesh and Uttar Pradesh, I don't know whether it's this state or that state. Whole night I've been riding... early morning around 6:30 I come and I park near a dhaba, tch, to have a tea or something. Motorcycles every 2000-3000 kilometers, those days these modern machines are better. You have to tighten the chain, chain becomes slack.\n",
    "\"I am always carrying an extra chain because I'm on the road. You just have to de-link... one link you have to take it off and put it, it's a small job but it's messy. It's greasy, oily, your hands go bad, everything. So I parked for tea in the morning. Then I saw there was one mechanic shop right there early morning. It was open. Mubarak Mechanical Works. I can't forget, this is a handwritten sign - Mubarak Mechanical Works. So I saw a young strapling (strapping?) youth, one Muslim boy. I called him. \"\"Hey, can you fix the chain? I just want one link to go.\"\" He said, \"\"Yes, I can do it.\"\" I said, \"\"Okay, do it.\"\" So he brought his tools. I looked at him. He's got a hammer and a chisel. I said, \"\"What, you are going to fix with that?\"\" He said, \"\"Yes.\"\" I said, \"\"Wait.\"\" And I walked into his garage, a small little shack. I look inside, all he is got is a hammer and chisel. With this he repairs everything (Laughs). \n",
    "\n",
    " Time 42:08\"\n",
    "I said, \"No, no, you're not going to touch my motorcycle (Laughs). Because I know if you do it with hammer and chisel after that nobody else can do anything with a motorcycle, it's finished.\" (Laughs). I said, \"No, you don't do it, I got all the tools with me\" (Laughs). I will do it.\" Why I'm saying this is like this, you read one book, as holy as the book maybe, you read one book, you are a hammer-and-chisel mechanic. With that you try to fix everything. No, I've read nothing spiritual, because spiritual is the basis of your existence. Spiritual is not in the book. Only this life can be spiritual, a book cannot be spiritual, a teaching cannot be spiritual, something else cannot be spiritual. Only this life can be spiritual.\n",
    "\n",
    "\n",
    "\"\"\"]:\n",
    "    instruction = f\"\"\"List the content type of the passage and all relevant tags:\n",
    "    Passage: {text}\"\"\"\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"<|im_start|>user\n",
    "    {instruction}<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    <think>\n",
    "\n",
    "    </think>\"\"\"\n",
    "\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False,  # Greedy first\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    print(\"Full output:\")\n",
    "    print(full_output.split('Type: ', 1)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd4d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r\"trained_models/markup_tags_16bit_r16_qwen3-1.7_lora\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"‚úÖ LoRA adapters saved to: {save_path}\")\n",
    "\n",
    "\n",
    "merged_model_path = save_path.replace('_lora', '_merged')\n",
    "model.save_pretrained_merged(\n",
    "    merged_model_path,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ 16-bit Merged model saved to: {merged_model_path}\")\n",
    "\n",
    "save_path = r\"trained_models/markup_tags_4bit_r16_qwen3-1.7_lora\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"‚úÖ LoRA adapters saved to: {save_path}\")\n",
    "\n",
    "\n",
    "merged_model_path = save_path.replace('_lora', '_merged')\n",
    "model.save_pretrained_merged(\n",
    "    merged_model_path,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_4bit\",\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ 4-bit Merged model saved to: {merged_model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a2627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WITH UNSLOTH - NEEDS GPU\n",
    "# 4-bit version better than 16-bit\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "import time\n",
    "\n",
    "lora4b = r\"trained_models/markup_tags_4bit_r16_qwen3-1.7_lora\"\n",
    "\n",
    "model4b, tokenizer4b = FastLanguageModel.from_pretrained(\n",
    "    model_name=lora4b,  # Your local directory\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,  \n",
    "    local_files_only=True,\n",
    "    \n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model4b)\n",
    "\n",
    "# Generate\n",
    "\n",
    "texts = [\"\"\"\"I must tell you my experience. There was a time I crisscrossed India on my motorcycle. Once again, I'm getting back on my motorcycle these days (Laughs). So I'm just riding somewhere between Madhya Pradesh and Uttar Pradesh, I don't know whether it's this state or that state. Whole night I've been riding... early morning around 6:30 I come and I park near a dhaba, tch, to have a tea or something. Motorcycles every 2000-3000 kilometers, those days these modern machines are better. You have to tighten the chain, chain becomes slack.\n",
    "\"I am always carrying an extra chain because I'm on the road. You just have to de-link... one link you have to take it off and put it, it's a small job but it's messy. It's greasy, oily, your hands go bad, everything. So I parked for tea in the morning. Then I saw there was one mechanic shop right there early morning. It was open. Mubarak Mechanical Works. I can't forget, this is a handwritten sign - Mubarak Mechanical Works. So I saw a young strapling (strapping?) youth, one Muslim boy. I called him. \"\"Hey, can you fix the chain? I just want one link to go.\"\" He said, \"\"Yes, I can do it.\"\" I said, \"\"Okay, do it.\"\" So he brought his tools. I looked at him. He's got a hammer and a chisel. I said, \"\"What, you are going to fix with that?\"\" He said, \"\"Yes.\"\" I said, \"\"Wait.\"\" And I walked into his garage, a small little shack. I look inside, all he is got is a hammer and chisel. With this he repairs everything (Laughs). \n",
    "\n",
    " Time 42:08\"\n",
    "I said, \"No, no, you're not going to touch my motorcycle (Laughs). Because I know if you do it with hammer and chisel after that nobody else can do anything with a motorcycle, it's finished.\" (Laughs). I said, \"No, you don't do it, I got all the tools with me\" (Laughs). I will do it.\" Why I'm saying this is like this, you read one book, as holy as the book maybe, you read one book, you are a hammer-and-chisel mechanic. With that you try to fix everything. No, I've read nothing spiritual, because spiritual is the basis of your existence. Spiritual is not in the book. Only this life can be spiritual, a book cannot be spiritual, a teaching cannot be spiritual, something else cannot be spiritual. Only this life can be spiritual.\n",
    "\n",
    "\n",
    "\"\"\", \"\"\"They were amazed how developed and how economically strong and so much wealth but unprotected. Their idea of a heap of gold means there would be ten men always standing there with arms. But here people put a heap of gold on the street and sold. Because society was so evolved, people understood what are the laws. If I steal yours, tomorrow you'll steal mine and the whole society will collapse after some time. So people were so wise and evolved when they came, this looked like a free-for-all bonanza. They could take what they want. Wealth was not protected, women were not protected. They were everywhere. They picked up what they want and exploited it in a most horrible way... most horrific way. Today we're trying to glorify those things also unfortunately but in a most horrific way they did this.\n",
    "\"When the Islamic invasions happened, this happened. They did not come as invaders. They did not come as religious crusaders. They just came as bandits. But when they saw it was so easy, because there were men involved in profound music, mathematics, astrolo... astronomy, astrology, all kinds of things but there were not too many fighting men. So it was a walk over. The land, the beautiful land that this was, the rich banks of the rivers, agriculturally rich, wealth, gold, diamonds... The first nation which mined diamonds in the world is India, all right. The greatest diamond in the world is still from India, it is sitting on the British crown (Laughs). But they saw this is like a paradise that you can take without resistance. And they took it because they were basically nomadic people, barbarian, they did their own terrible things. Even when the British came, we must understand it is not the United Kingdom's army which came, its East India Company which came. It's a business which came but they saw it's so easy to take this country. They became emperors (Laughs) over a period of time. \n",
    "\n",
    " Time 26:26\"\n",
    "\"\"\", \"\"\"\"See twenty-eight years is the yoga center. This activity is on for thirty-seven years now. But largely till '98, '99 or 2000, till 2000, we were only focused on one goal, Dhyanalinga. So really our worldwide activity is only from 2001 till now. So it's actually eighteen years of activity. Well, today people estimate that we have touched over five-hundred million people, that's half a billion people. But thirty-seven years ago, when this phenomenal experience happened to me on the Chamundi hills, I sat there and simply if I sit there I am... I am like dripping ecstasy in every cell in my body. I again and again tried the next few days and weeks that I do activity something and if I simply sit, it's... I am just bursting with ecstasy. \n",
    "\n",
    " Time 10:08 \n",
    "\n",
    " That time I thought (Laughs) that this is so simple. If you don't do anything... If you don't mess with yourself, you will be ecstatic. Ecstatic means bursting with ecstasy. The peak of your life, if only you don't mess with your psychological stuff. Then I thought this is so simple. I will make the whole world ecstatic (Laughs). Well, at that moment, I thought this is the first time I have discovered this because I had no traditional background, I had no spiritual background, I just grew up in a very westernized way. And this was bursting within me, I thought... That is the time where, you know, youth of that... that generation were so much into drugs, seeking experiences. But here I was, if I just close my eyes, I'm in the highest possible experience that a human being can be. So I thought no drug, no God, no religion, no nothing, no philosophy, if you don't mess with yourself, you're ecstatic. \n",
    "\n",
    " I thought this is very simple. On that day, world's population was 5.6 billion people. I thought... I made a plan. In two-and-a-half years' time, I'll make the whole world ecstatic (Laughs).\"\n",
    "\"Interviewer: Thirty-eight years... \n",
    "\n",
    " Sadhguru: (Laughs). People think we are a great success because we've touched half a billion people. I don't think so - because for me, humanity means 7.6 billion people. Well, I might die a failure (Laughs) but a blissful failure. This is something all the young people must understand. You must... You must choose to fail because the goals that you choose are so huge; in one lifetime, you cannot do it. You setup petty goals, you finish that and you think you're a great success, what is the point of such a life? You aspire for something which cannot be fulfilled by one generation of people. But you have the privilege of setting the direction for that. You will die of failure but it's a very blissful failure (Laughs). \n",
    "\n",
    " So as far as I'm concerned, I am a failure. Isha Foundation is a failure. It doesn't matter, all of us work nearly twenty hours a day, seven days of the week but all the time we know we are short of what we can do or what we should be doing. This is what drives everybody here. All the young people who are here, they have known usual pleasures of life, what others are seeking in cities and all. The only pleasure they have is they're working for a large vision. And they see the transformation in people's faces, how they come and how they leave, you must see. You should have recorded - people how they come on day one to a program of three days, when they leave, they're bursting with ecstasy. So like this millions of people seeing their faces and seeing the transformation that is being brought about, that is the only thing which keeps them going and their own transformation also of course. So these thirty-seven years, though in my view, it's a failure, the world thinks we are a great success (Laughs) because they think small. \n",
    "\n",
    " Time 14:11\"\n",
    "\"\"\"]\n",
    "\n",
    "timh = time.time()\n",
    "for text in texts:\n",
    "    instruction = f\"\"\"List the content type of the passage and all relevant tags:\n",
    "    Passage: {text}\"\"\"\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"<|im_start|>user\n",
    "    {instruction}<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    <think>\n",
    "\n",
    "    </think>\"\"\"\n",
    "\n",
    "    inputs = tokenizer4b(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model4b.generate(**inputs, max_new_tokens=50,\n",
    "        temperature=0.01,  # Low temperature for deterministic output\n",
    "        top_p=0.95,\n",
    "        do_sample=True,)\n",
    "    print(tokenizer4b.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "print(time.time()-timh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora16b = r\"trained_models/markup_tags_16bit_r16_qwen3-1.7_lora\"\n",
    "\n",
    "model16b, tokenizer16b = FastLanguageModel.from_pretrained(\n",
    "    model_name=lora16b,  # Your local directory\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=False,  \n",
    "    local_files_only=True,\n",
    "    \n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model4b)\n",
    "\n",
    "timh = time.time()\n",
    "for text in texts:\n",
    "    instruction = f\"\"\"List the content type of the passage and all relevant tags:\n",
    "    Passage: {text}\"\"\"\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"<|im_start|>user\n",
    "    {instruction}<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    <think>\n",
    "\n",
    "    </think>\"\"\"\n",
    "\n",
    "    inputs = tokenizer16b(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model16b.generate(**inputs, max_new_tokens=50,\n",
    "        temperature=0.01,  # Low temperature for deterministic output\n",
    "        top_p=0.95,\n",
    "        do_sample=True,)\n",
    "    print(tokenizer16b.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "print(time.time()-timh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7680f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora16b = r\"trained_models/markup_tags_16bit_r16_qwen3-1.7_lora\"\n",
    "\n",
    "model16b, tokenizer16b = FastLanguageModel.from_pretrained(\n",
    "    model_name=lora16b,  # Your local directory\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=False,  \n",
    "    local_files_only=True,\n",
    "    \n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model4b)\n",
    "\n",
    "timh = time.time()\n",
    "for text in texts:\n",
    "    instruction = f\"\"\"List the content type of the passage and all relevant tags:\n",
    "    Passage: {text}\"\"\"\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"<|im_start|>user\n",
    "    {instruction}<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    <think>\n",
    "\n",
    "    </think>\"\"\"\n",
    "\n",
    "    inputs = tokenizer16b(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model16b.generate(**inputs, max_new_tokens=50,\n",
    "        temperature=0.01,  # Low temperature for deterministic output\n",
    "        top_p=0.95,\n",
    "        do_sample=True,)\n",
    "    print(tokenizer16b.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "print(time.time()-timh)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
